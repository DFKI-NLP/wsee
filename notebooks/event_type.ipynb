{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the data to work with Snorkel: Part 1 - Event Type\n",
    "\n",
    "Essentially we will have to create two labeling models.\n",
    "One assigns labels to event types and the other assigns labels to argument roles in event mentions.\n",
    "\n",
    "In any case we need to create a row for each event (trigger) to do event type labeling.\n",
    "\n",
    "For this we need 1 additional column:\n",
    "- trigger_id\n",
    "\n",
    "One numpy array containing the:\n",
    "- event_type\n",
    "\n",
    "We will probably focus on keyword lists and some heuristics to create our labeling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wsee.utils import utils\n",
    "from wsee.data import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='once')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "DATA_DIR = '/Users/phuc/data/daystream_first_trigger_check'  # replace path to corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SD4M Event Types\n",
    "\n",
    "| Number | Code                   | Description                                                                             |\n",
    "|--------|------------------------|-----------------------------------------------------------------------------------------|\n",
    "| -1     | ABSTAIN                | No vote, for Labeling Functions                                                         |\n",
    "| 0      | Accident               | Collision of a vehicle with another vehicle, person, or obstruction                     |\n",
    "| 1      | CanceledRoute          | Cancellation of public transport routes                                                 |\n",
    "| 2      | CanceledStop           | Cancellation of public transport stops                                                  |\n",
    "| 3      | Delay                  | Delay resulting from remaining traffic disturbances                                     |\n",
    "| 4      | Obstruction            | Temporary installation to control traffic                                               |\n",
    "| 5      | RailReplacementService | Replacement of a passenger train by buses or other substitute public transport services |\n",
    "| 6      | TrafficJam             | Line of stationary or very slow-moving traffic                                          |\n",
    "| 7      | O                      | No SD4M event.                                                                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = pipeline.load_data(DATA_DIR)\n",
    "sd_train = loaded_data['train']\n",
    "sd_dev = loaded_data['dev']\n",
    "sd_test = loaded_data['test']\n",
    "\n",
    "daystream = loaded_data['daystream']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create one row for every event trigger\n",
    "\n",
    "We will use the (labeled) SD4M training set as our development data to create our labeling functions.\n",
    "In this notebook we will run our labeling functions and our LabelModel on that data.\n",
    "In the real pipeline we will instead label the Daystream data that does not have event type and event argument role labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev, Y_dev = pipeline.build_event_trigger_examples(sd_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the (labeled) SD4m development set as our \"test set\" to measure the performance of our LabelModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test, Y_test = pipeline.build_event_trigger_examples(sd_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wsee import SD4M_RELATION_TYPES\n",
    "print(SD4M_RELATION_TYPES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wsee.preprocessors.preprocessors import *\n",
    "from wsee.data import explore, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply all our preprocessors on our data and see if we can find something interesting for our labeling functions.\n",
    "Let's first sample the SD4M training data, which is labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_sd4m_triggers = explore.add_labels(df_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_sd4m_triggers = explore.apply_preprocessors(labeled_sd4m_triggers, [pre_trigger_left_tokens, pre_mixed_ner, pre_trigger_right_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_sd4m_triggers = explore.add_event_types(labeled_sd4m_triggers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can collect the trigger words per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "filtered_sd4m_triggers = labeled_sd4m_triggers[labeled_sd4m_triggers['label'] != 7]\n",
    "print(f\"Number of events: {len(labeled_sd4m_triggers)}\\n\")\n",
    "for idx, class_name in enumerate(SD4M_RELATION_TYPES):\n",
    "    class_sd4m_triggers = labeled_sd4m_triggers[labeled_sd4m_triggers['label'] == idx]\n",
    "    print(f\"{class_name}: {len(class_sd4m_triggers)} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate the labeling functions on the SD4M training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wsee.labeling import event_trigger_lfs as trigger_lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "lfs = [\n",
    "    trigger_lfs.lf_accident_context,\n",
    "    trigger_lfs.lf_accident_context_street,\n",
    "    trigger_lfs.lf_accident_context_no_cause_check,\n",
    "    trigger_lfs.lf_canceledroute_cat,\n",
    "    trigger_lfs.lf_canceledroute_amplifier1,\n",
    "    trigger_lfs.lf_canceledstop_cat,\n",
    "    trigger_lfs.lf_canceledstop_amplifier1,\n",
    "    trigger_lfs.lf_delay_cat,\n",
    "    trigger_lfs.lf_delay_priorities,\n",
    "    trigger_lfs.lf_delay_duration,\n",
    "    trigger_lfs.lf_obstruction_cat,\n",
    "    trigger_lfs.lf_obstruction_street,\n",
    "    trigger_lfs.lf_obstruction_priorities,\n",
    "    trigger_lfs.lf_railreplacementservice_cat,\n",
    "    trigger_lfs.lf_railreplacementservice_amplifier1,\n",
    "    trigger_lfs.lf_trafficjam_cat,\n",
    "    trigger_lfs.lf_trafficjam_street,\n",
    "    trigger_lfs.lf_trafficjam_order,\n",
    "    trigger_lfs.lf_negative,\n",
    "    trigger_lfs.lf_cause_negative,\n",
    "    trigger_lfs.lf_obstruction_negative\n",
    "]\n",
    "\n",
    "applier = PandasLFApplier(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_dev = applier.apply(df_dev)\n",
    "L_test = applier.apply(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L_dev, lfs).lf_summary(Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Error Analysis \n",
    "Now we can look at the LabelMatrix for errors. We need to use the DataFrame from the exploration section, which includes the information from the preprocessors.\n",
    "We can then specifically look for the instances that were labeled incorrectly.\n",
    "\n",
    "We will first look at the keyword based labeling function for accidents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wsee.labeling import error_analysis\n",
    "relevant_cols = ['text','trigger', 'event_types']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_sd4m_triggers.iloc[L_dev[:, 3] == 1].sample(5)[['text', 'trigger', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):  # more options can be specified also\n",
    "    display(error_analysis.get_false_positives(labeled_df=labeled_sd4m_triggers, lf_outputs=L_dev, lf_index=3, label_of_interest=1)[relevant_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis.get_abstained_instances(labeled_df=labeled_sd4m_triggers, lf_outputs=L_dev, lf_index=9, label_of_interest=4)[relevant_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the Labeling model and label the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd4m_trigger_class_balance = [\n",
    "    0.07221542227662178,\n",
    "    0.07466340269277846,\n",
    "    0.030599755201958383,\n",
    "    0.0795593635250918,\n",
    "    0.12362301101591187,\n",
    "    0.02692778457772338,\n",
    "    0.189718482252142,\n",
    "    0.40269277845777235\n",
    "]\n",
    "\n",
    "daystream_trigger_lf_class_balance = [\n",
    "    0.05769230769230769,\n",
    "    0.24038461538461536,\n",
    "    0.028846153846153844,\n",
    "    0.2019230769230769,\n",
    "    0.19230769230769232,\n",
    "    0.09615384615384616,\n",
    "    0.028846153846153844,\n",
    "    0.15384615384615385\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelModel\n",
    "\n",
    "label_model = LabelModel(cardinality=8, verbose=True)\n",
    "label_model.fit(L_train=L_dev, n_epochs=500, log_freq=100, seed=123, class_balance=sd4m_trigger_class_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_model_acc = label_model.score(L=L_dev, Y=Y_dev, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_train = label_model.predict_proba(L=L_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the proposed workflow one would filter out all the datapoints that were not labeled by any of the labeling functions.\n",
    "We will follow this approach as that does not affect the merging process in our Snorkel processing pipeline.\n",
    "While it may result in sentences missing certain events, they would then be processed as dummy events in the AllenNLP model and factored out during the loss calculation (?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "    X=df_dev, y=probs_train, L=L_dev\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Snorkel processing pipeline we would merge the labeled dataframes back together that belong to the same document and proceed with labeling the event argument roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_sd_train = pipeline.merge_event_trigger_examples(df_train_filtered.drop('event_triggers', axis=1), probs_train_filtered)\n",
    "labeled_sd_train.reset_index(level=0).to_json(DATA_DIR + \"/save_sd_triggers.jsonl\", orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Label the Daystream data with Snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, Y_train = pipeline.build_event_trigger_examples(daystream)\n",
    "L_train = applier.apply(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFAnalysis(L_train, lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daystream_model = LabelModel(cardinality=8, verbose=True)\n",
    "daystream_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=123, class_balance=daystream_trigger_lf_class_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daystream_model_acc = daystream_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Label Model Accuracy:':<25} {daystream_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daystream_probs = daystream_model.predict_proba(L=L_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "    X=df_train, y=daystream_probs, L=L_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_daystream = pipeline.merge_event_trigger_examples(df_train_filtered, probs_train_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_daystream.reset_index(level=0).to_json(DATA_DIR + \"/save_daystreamv6_triggers.jsonl\", orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Check Daystream Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['trigger_probs'] = list(daystream_probs)\n",
    "df_train['most_probable_class'] = [SD4M_RELATION_TYPES[label_idx] for label_idx in daystream_probs.argmax(axis=1)]\n",
    "df_train['max_class_prob'] = [\"{:.2f}\".format(class_prob) for class_prob in daystream_probs.max(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trigger_class in SD4M_RELATION_TYPES:\n",
    "    print(f\"{trigger_class}: {len(df_train[df_train['most_probable_class'] == trigger_class])} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to display all the rows of the dataframe:\n",
    "```python\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(df_train[df_train['most_probable_class'] == 'O'][['text', 'trigger', 'most_probable_class', 'max_class_prob', 'trigger_probs']])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['most_probable_class'] == 'Obstruction'].sample(10)[['text', 'trigger', 'most_probable_class', 'max_class_prob', 'trigger_probs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (master)",
   "language": "python",
   "name": "master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
