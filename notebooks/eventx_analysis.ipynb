{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "from wsee.predictors.predictor_utils import load_predictor\n",
    "from wsee.models.model_utils import batched_predict_json\n",
    "from wsee.utils import scorer, evaluate\n",
    "from wsee import SD4M_RELATION_TYPES, ROLE_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEVICE = -1  # or -1 if no GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change paths\n",
    "# DATASET_PATH = \"../../data/daystream_corpus/test/test_sd4m_with_events.jsonl\"\n",
    "DATASET_PATH = \"/Users/phuc/develop/python/eventx/data/daystream_corpus/test/test_sd4m_with_events.jsonl\"\n",
    "PREDICTOR_NAME = \"snorkel-eventx-predictor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test data, models and do predictions\n",
    "\n",
    "Filter documents that do not contain any entity of type trigger as they are not relevant for the event extraction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = evaluate.load_test_data(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_base_path = \"../../data/runs/\"\n",
    "model_base_path = \"/Users/phuc/develop/python/eventx/data/runs/\"\n",
    "model_names = [\n",
    "    \"snorkel_bert_gold\",\n",
    "    \"snorkel_bert_daystream\",\n",
    "    \"snorkel_bert_merged\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_docs = {}\n",
    "for model_name in model_names:\n",
    "    predictor = load_predictor(model_base_path + model_name, PREDICTOR_NAME, CUDA_DEVICE)\n",
    "    predicted_docs[model_name] = batched_predict_json(predictor=predictor, examples=test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional evaluation using sklearn toolkit\n",
    "We simply consider the trigger & role label sequences disregarding the correctness of the corresponding trigger label when evaluating the argument roles to use the sklearn toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_y_pred, arg_y_pred = {}, {}\n",
    "for model_name in model_names:\n",
    "    label_arrays = evaluate.get_label_arrays(test_docs, predicted_docs[model_name])\n",
    "    trigger_y_true, trigger_y_pred[model_name] = label_arrays[\"trigger_y_true\"], label_arrays[\"trigger_y_pred\"]\n",
    "    arg_y_true, arg_y_pred[model_name] = label_arrays[\"arg_y_true\"], label_arrays[\"arg_y_pred\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model trained with SD4M gold training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(trigger_y_true, trigger_y_pred[\"snorkel_bert_gold\"], SD4M_RELATION_TYPES[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(trigger_y_true, trigger_y_pred[\"snorkel_bert_daystream\"], SD4M_RELATION_TYPES[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(trigger_y_true, trigger_y_pred[\"snorkel_bert_merged\"], SD4M_RELATION_TYPES[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 3, figsize=(30,7))\n",
    "f.suptitle('Normalized Confusion Matrices - Trigger classification')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    \n",
    "    cm = sklearn.metrics.confusion_matrix(trigger_y_true, trigger_y_pred[model_name], labels=SD4M_RELATION_TYPES)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    df_cm = pd.DataFrame(cm, index = SD4M_RELATION_TYPES, columns = SD4M_RELATION_TYPES)\n",
    "    sns.heatmap(df_cm, ax=axes[i], annot=True)\n",
    "    axes[i].set_title(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argument role classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(arg_y_true, arg_y_pred[\"snorkel_bert_gold\"], ROLE_LABELS[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(arg_y_true, arg_y_pred[\"snorkel_bert_daystream\"], ROLE_LABELS[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(arg_y_true, arg_y_pred[\"snorkel_bert_merged\"], ROLE_LABELS[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 3, figsize=(30,7))\n",
    "f.suptitle('Normalized Confusion Matrices - Argument classification')\n",
    "for i, model_name in enumerate(model_names):\n",
    "    \n",
    "    cm = sklearn.metrics.confusion_matrix(arg_y_true, arg_y_pred[model_name], labels=ROLE_LABELS)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    df_cm = pd.DataFrame(cm, index=ROLE_LABELS, columns=ROLE_LABELS)\n",
    "    sns.heatmap(df_cm, ax=axes[i], annot=True)\n",
    "    axes[i].set_title(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event extraction evaluation using correctness criteria defined by Ji, Heng and Grishman, Ralph 2008\n",
    "\n",
    "Ji, Heng and Grishman, Ralph (2008). Refining event extraction through cross-document inference.\n",
    "> - A trigger is correctly labeled if its event type and offsets match a reference trigger.\n",
    "> - An argument is correctly identified if its event type and offsets match any of the reference argument mentions.\n",
    "> - An argument is correctly identified and classified if its event type, offsets, and role match any of the reference argument mentions.\n",
    "\n",
    "The main difference to the previous evaluation method lies in the correctness criteria for the arguments. Here we additionally consider the correctness of the event type for the argument.\n",
    "\n",
    "Caution:\n",
    "Using the following methods to retrieve the triggers and arguments from the gold data might result in duplicate gold triggers & arguments.\n",
    "This is due to different events possibly sharing the same trigger.\n",
    "The model is not able to distinguish such events and instead fuses them all together, which results in lower recall.\n",
    "If we remove duplicates from the gold triggers and gold arguments, recall and consequently f1 should be higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_DUPLICATES = True  # change to False if you want to keep duplicate triggers/ arguments from the gold data caused by events sharing the same trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_triggers = scorer.get_triggers(test_docs)\n",
    "gold_arguments = scorer.get_arguments(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REMOVE_DUPLICATES:\n",
    "    gold_triggers = list(set(gold_triggers))\n",
    "    gold_arguments = list(set(gold_arguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_triggers = {}\n",
    "pred_arguments = {}\n",
    "for model_name in model_names:\n",
    "    pred_triggers[model_name] = scorer.get_triggers(predicted_docs[model_name])\n",
    "    pred_arguments[model_name] = scorer.get_arguments(predicted_docs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    trigger_id_metrics = scorer.get_trigger_identification_metrics(gold_triggers, pred_triggers[model_name], output_string=True)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    trigger_class_metrics = scorer.get_trigger_classification_metrics(gold_triggers, pred_triggers[model_name], output_string=True)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    argument_id_metrics = scorer.get_argument_identification_metrics(gold_arguments, pred_arguments[model_name], output_string=True)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    print(model_name)\n",
    "    argument_class_metrics = scorer.get_argument_classification_metrics(gold_arguments, pred_arguments[model_name], output_string=True)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Random Repeats\n",
    "5 random repeats for each configuration with random seeds for the snorkel label models and eventx model.\n",
    "Metrics are given as median & standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base_path = Path(model_base_path)\n",
    "runs = 5\n",
    "trigger_metrics, argument_metrics = {}, {}\n",
    "for model_name in model_names:\n",
    "    model_paths = [model_base_path.joinpath(f'run0{run+1}/{model_name}') for run in range(runs)]\n",
    "    trigger_metrics[model_name], argument_metrics[model_name] = evaluate.summize_multiple_runs(model_paths, test_docs)\n",
    "    print(model_name)\n",
    "    print(pd.DataFrame(trigger_metrics[model_name]))\n",
    "    print(pd.DataFrame(argument_metrics[model_name]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalability of snorkel labeled data\n",
    "Compare model performance with increasing amount of snorkel labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [f'snorkel_bert_daystream{percentage}' for percentage in range(50,101,10)]\n",
    "model_names.append('snorkel_bert_daystream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_triggers = {}\n",
    "pred_arguments = {}\n",
    "trigger_id_metrics = {}\n",
    "trigger_class_metrics = {}\n",
    "argument_id_metrics = {}\n",
    "argument_class_metrics = {}\n",
    "for model_name in model_names:\n",
    "    predictor = load_predictor(model_base_path.joinpath(model_name), PREDICTOR_NAME, CUDA_DEVICE)\n",
    "    predicted_docs[model_name] = batched_predict_json(predictor=predictor, examples=test_docs)\n",
    "    pred_triggers[model_name] = scorer.get_triggers(predicted_docs[model_name])\n",
    "    pred_arguments[model_name] = scorer.get_arguments(predicted_docs[model_name])\n",
    "    print(model_name)\n",
    "    trigger_id_metrics[model_name] = scorer.get_trigger_identification_metrics(gold_triggers, pred_triggers[model_name], output_string=False)\n",
    "    trigger_class_metrics[model_name] = scorer.get_trigger_classification_metrics(gold_triggers, pred_triggers[model_name], output_string=False) # restrict to Trigger classification\n",
    "    argument_id_metrics[model_name] = scorer.get_argument_identification_metrics(gold_arguments, pred_arguments[model_name], output_string=False)\n",
    "    argument_class_metrics[model_name] = scorer.get_argument_classification_metrics(gold_arguments, pred_arguments[model_name], output_string=False) # restrict to Argument classification\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger_id_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_infos(metrics, metric_name):\n",
    "    table = []\n",
    "    for k,v in metrics.items():\n",
    "        tmp = {'Model': k, metric_name: v[metric_name]['f1-score']}\n",
    "        table.append(tmp)\n",
    "    return pd.DataFrame(table).set_index('Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progression_table = get_infos(trigger_id_metrics, 'Trigger identification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progression_table = progression_table.merge(get_infos(trigger_class_metrics, 'Trigger classification'), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progression_table = progression_table.merge(get_infos(argument_id_metrics, 'Argument identification'), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progression_table = progression_table.merge(get_infos(argument_class_metrics, 'Argument classification'), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progression_table.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python wsee_updated)",
   "language": "python",
   "name": "wsee_updated"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}