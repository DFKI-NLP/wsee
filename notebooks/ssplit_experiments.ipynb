{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy, stanfordnlp, SoMaJo comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_spacy = spacy.load('de_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "# stanfordnlp.download('de')\n",
    "nlp_stanford = stanfordnlp.Pipeline(lang='de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from somajo import SoMaJo\n",
    "tokenizer = SoMaJo(\"de_CMC\", split_camel_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"#S1 Nach der Weichenstörung in Hohen Neuendorf verkehren die S-Bahnen wieder durchgehend, erster Zug ab #Frohnau 21:58 Uhr und erster Zug ab #Hohen_Neuendorf 22:03 Uhr.\"\n",
    "doc2 = \"Unfall\\nAbschnitt: Marzahn (Berlin)\\nGültig ab: 09.02.2016 20:06\\ngesperrt, Unfall\\n\"\n",
    "doc3 = \"■ #A1 #Bremen Richtung #Hamburg zwischen Horster Dreieck und #Stillhorn 9 km #Stau.  Dort ist wegen #Bauarbeiten nur eine Spur frei.\\n\"\n",
    "doc4 = \"Wegen einer techn. Störung an der Strecke besteht für die Linien S41, S42 u. S46 zw. Halensee <> Westkreuz <> Messe Nord <> Westend S-Bahn-Pendelverkehr im 20-Minuten-Takt. Die Linien S41 u. S42 fahren nur im 10-Minuten-Takt, die Linie S46 fährt nur Königs Wusterhausen <> Tempelhof.\"\n",
    "doc5 = \"#S3, #S5, #S7, #S9: Nach einer ärztliche Versorgung eines Fahrgastes im Zug in Bellevue kommt es noch zu Verspätungen und vereinzelten Ausfällen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = [doc1, doc2, doc3, doc4, doc5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process documents with spaCy, stanfordnlp, somajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_docs = [nlp_spacy(doc) for doc in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_docs = [nlp_stanford(doc) for doc in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somajo_docs = [list(tokenizer.tokenize_text([doc])) for doc in test_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization comparison\n",
    "How to access tokens:\n",
    "\n",
    "#### spaCy\n",
    "`Doc` is a sequence of `Token`s. We can get the token text with `Token.text`.\n",
    "\n",
    "#### stanfordnlp\n",
    "Here we have to access the sentences of a `Doc` to access the tokens with `tokens` property. We can get the token text with `Token.text`.\n",
    "\n",
    "### somajo\n",
    "Similar to stanfordnlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_doc_tokens(doc):\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def get_stanford_doc_tokens(doc):\n",
    "    return [token.text for sentence in doc.sentences for token in sentence.tokens]\n",
    "\n",
    "def get_somajo_doc_tokens(doc):\n",
    "    return [token.text for sentence in doc for token in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for spacy_doc, stanford_doc, somajo_doc in zip(spacy_docs, stanford_docs, somajo_docs):\n",
    "    spacy_tokens = get_spacy_doc_tokens(spacy_doc)\n",
    "    print(\"spaCy:\", spacy_tokens)\n",
    "    stanford_tokens = get_stanford_doc_tokens(stanford_doc)\n",
    "    print(\"stanfordnlp:\", stanford_tokens)\n",
    "    somajo_tokens = get_somajo_doc_tokens(somajo_doc)\n",
    "    print(\"somajo:\", somajo_tokens)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy tokenizer treats hashtags as separate tokens and keeps whitespace characters.\n",
    "stanfordnlp more often than not treats hashtags as separate token and often does not handle abbreviations well, i.e. the tokenizer treats the dot as a separate token.\n",
    "It also tends to split words containing punctuation marks more aggressively than the other tokenizers.\n",
    "SoMaJo does not treat hashtags as separate tokens and handles abbreviations better. It does however split dates into multiple tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence splitting comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_doc_sentences(doc):\n",
    "    return [s.text for s in doc.sents]\n",
    "\n",
    "def get_stanford_doc_sentences(doc):\n",
    "    # introduces whitespaces\n",
    "    # see: https://github.com/stanfordnlp/stanfordnlp/blob/dev/stanfordnlp/models/common/doc.py\n",
    "    # to get original sentence text\n",
    "    return [\" \".join([token.text for token in sentence.tokens]) for sentence in doc.sentences]\n",
    "\n",
    "def get_somajo_doc_sentences(doc):\n",
    "    # introduces whitespaces\n",
    "    return [\" \".join([token.text for token in sentence]) for sentence in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for spacy_doc, stanford_doc, somajo_doc in zip(spacy_docs, stanford_docs, somajo_docs):\n",
    "    spacy_sentences = get_spacy_doc_sentences(spacy_doc)\n",
    "    print(\"spaCy:\", len(spacy_sentences), \"\\n\", spacy_sentences)\n",
    "    stanford_sentences = get_stanford_doc_sentences(stanford_doc)\n",
    "    print(\"stanfordnlp:\", len(stanford_sentences), \"\\n\", stanford_sentences)\n",
    "    somajo_sentences = get_somajo_doc_sentences(somajo_doc)\n",
    "    print(\"somajo:\", len(somajo_sentences), \"\\n\", somajo_sentences)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the small sample of sentences we can observe that spaCy tends to split the document text very aggressively. It seems to not be able to handle hashtags, punctuation marks and abbreviations well.\n",
    "stanfordnlp tends to do a little better, but seems rather ill-equipped to handle text data from social media containing a lot of abbreviations and use of special punctuation marks.\n",
    "SoMaJo does considerably better. In our testing we found that it only made mistakes on very few occasions where it encountered unknown abbreviations.\n",
    "Therefore we chose to do event extraction on a document level and use SoMaJo sentence splitting information for our negative labeling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wsee.utils import corpus_statistics\n",
    "sd4m_train = pd.read_json(\"../data/daystream_corpus/train/train_with_events_and_defaults.jsonl\", lines=True, encoding='utf8')\n",
    "filtered_sd4m_train = sd4m_train[sd4m_train.apply(lambda document: corpus_statistics.has_triggers(document), axis=1)]\n",
    "corpus_statistics.get_snorkel_event_stats(filtered_sd4m_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wsee.data import pipeline\n",
    "\n",
    "df_sd_train, Y_sd_train = pipeline.build_event_role_examples(filtered_sd4m_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wsee.labeling import event_argument_role_lfs as role_lfs\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "lfs = [\n",
    "    role_lfs.lf_somajo_separate_sentence,\n",
    "    role_lfs.lf_stanford_separate_sentence\n",
    "]\n",
    "applier = PandasLFApplier(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_sd_train = applier.apply(df_sd_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L_sd_train, lfs).lf_summary(Y_sd_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SD4M train set contains 2001 positive event roles, but `lf_stanford_separate_sentenc` using the sentence splitting information from stanfordnlp incorrectly labels 94 of these as `no_arg`.\n",
    "While `lf_somajo_separate_sentenc` identifies less correct `no_arg` instances, it only rarely labels event_roles incorrectly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wsee.labeling import error_analysis\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "labeled_sd4m_roles = df_sd_train.copy()\n",
    "labeled_sd4m_roles['label'] = Y_sd_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis.sample_fp(labeled_df=labeled_sd4m_roles, lf_outputs=L_sd_train, lf_index=0, label_of_interest=10, sample_size=1)[['text', 'trigger', 'argument', 'somajo_doc', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis.sample_fp(labeled_df=labeled_sd4m_roles, lf_outputs=L_sd_train, lf_index=1, label_of_interest=10, sample_size=1)[['text', 'trigger', 'argument', 'sentence_spans', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (master)",
   "language": "python",
   "name": "master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
