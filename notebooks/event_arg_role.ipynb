{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the data to work with Snorkel: Part 2 - Event Role\n",
    "\n",
    "Here we will do most of the work creating a labeling model that assigns labels to argument roles in event mentions.\n",
    "We need to create a row for each pair of trigger and entity mention.\n",
    "\n",
    "For this we need to create 2 additional columns:\n",
    "- trigger_id\n",
    "- argument_id\n",
    "\n",
    "Everything else we can pull from the other columns using Snorkel preprocessor functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from wsee.utils import utils\n",
    "from wsee.data import pipeline\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "DATA_DIR = '/Users/phuc/data/daystream_corpus'  # replace path to corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SD4M Relation/ Event Arguments\n",
    "\n",
    "| Number | Code       | Description                                                                 |\n",
    "|--------|------------|-----------------------------------------------------------------------------|\n",
    "| -1     | ABSTAIN    | No vote, for Labeling Functions                                             |\n",
    "| 0      | location   | Required argument for all events denoting the location.                     |\n",
    "| 1      | delay      | Optional argument denoting the delay associated with the event.             |\n",
    "| 2      | direction  | Optional argument denoting the direction associated with the event.         |\n",
    "| 3      | start_loc  | Optional argument denoting the starting location associated with the event. |\n",
    "| 4      | end_loc    | Optional argument denoting the ending location associated with the event.   |\n",
    "| 5      | start_date | Optional argument denoting the start date associated with the event.        |\n",
    "| 6      | end_date   | Optional argument denoting the end date associated with the event.          |\n",
    "| 7      | cause      | Optional argument (trigger) denoting the cause associated with the event.   |\n",
    "| 8      | jam_length | Optional argument denoting the jam length of a traffic jam event.           |\n",
    "| 9      | route      | Optional argument denoting the route affected by a canceled stop event.     |\n",
    "| 10     | no_arg     | No argument relation with the specified trigger.                            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = pipeline.load_data(DATA_DIR)\n",
    "sd_train = loaded_data['train']\n",
    "sd_dev = loaded_data['dev']\n",
    "sd_test = loaded_data['test']\n",
    "\n",
    "daystream = loaded_data['daystream']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create one row for each trigger-entity pair (event role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_file = DATA_DIR + '/pickled_sd_train_role_examples'\n",
    "pickled_dataframe_file = Path(dataframe_file + '.pkl')\n",
    "\n",
    "df_sd_train = None\n",
    "Y_sd_train = None\n",
    "\n",
    "if pickled_dataframe_file.exists():\n",
    "    with open(pickled_dataframe_file, 'rb') as pickled_dataframe:\n",
    "        df_sd_train, Y_sd_train = pickle.load(pickled_dataframe)\n",
    "else:\n",
    "    df_sd_train, Y_sd_train = pipeline.build_event_role_examples(sd_train)\n",
    "    with open(pickled_dataframe_file, 'wb') as pickled_dataframe:\n",
    "        pickle.dump((df_sd_train, Y_sd_train), pickled_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_file = DATA_DIR + '/pickled_sd_dev_role_examples'\n",
    "pickled_dataframe_file = Path(dataframe_file + '.pkl')\n",
    "\n",
    "df_sd_dev = None\n",
    "Y_sd_dev = None\n",
    "\n",
    "if pickled_dataframe_file.exists():\n",
    "    with open(pickled_dataframe_file, 'rb') as pickled_dataframe:\n",
    "        df_sd_dev, Y_sd_dev = pickle.load(pickled_dataframe)\n",
    "else:\n",
    "    df_sd_dev, Y_sd_dev = pipeline.build_event_role_examples(sd_dev)\n",
    "    with open(pickled_dataframe_file, 'wb') as pickled_dataframe:\n",
    "        pickle.dump((df_sd_dev, Y_sd_dev), pickled_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wsee import ROLE_LABELS\n",
    "print(ROLE_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wsee.preprocessors.preprocessors import *\n",
    "from wsee.data import explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply all our preprocessors on our data and see if we can find something interesting for our labeling functions. Let's first sample the SD4M training data, which is labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_file = DATA_DIR + '/pickled_labeled_sd4m_role_examples'\n",
    "pickled_dataframe_file = Path(dataframe_file + '.pkl')\n",
    "\n",
    "labeled_sd4m_roles = None\n",
    "\n",
    "if pickled_dataframe_file.exists():\n",
    "    with open(pickled_dataframe_file, 'rb') as pickled_dataframe:\n",
    "        labeled_sd4m_roles = pickle.load(pickled_dataframe)\n",
    "else:\n",
    "    labeled_sd4m_roles = explore.add_labels(df_sd_train, Y_sd_train)\n",
    "    labeled_sd4m_roles = explore.apply_preprocessors(labeled_sd4m_roles, [pre_between_tokens, pre_between_distance])\n",
    "    labeled_sd4m_roles = explore.add_event_types(labeled_sd4m_roles)\n",
    "    labeled_sd4m_roles = explore.add_event_arg_roles(labeled_sd4m_roles)\n",
    "    with open(pickled_dataframe_file, 'wb') as pickled_dataframe:\n",
    "        pickle.dump(labeled_sd4m_roles, pickled_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at the trigger and argument text, and the entity types!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore.sample_data(labeled_sd4m_roles[labeled_sd4m_roles['label']==6], columns=['text', 'between_tokens', 'trigger', 'argument', 'between_distance', 'label', 'event_types', 'event_arg_roles'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can collect the most frequent trigger-argument pairs per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "filtered_sd4m_roles = labeled_sd4m_roles[labeled_sd4m_roles['label'] != 10]\n",
    "class_pairs = {}\n",
    "print(f\"Number of event-roles: {len(labeled_sd4m_roles)}\\n\")\n",
    "for idx, class_name in enumerate(ROLE_LABELS):\n",
    "    class_sd4m_roles = labeled_sd4m_roles[labeled_sd4m_roles['label'] == idx]\n",
    "    print(f\"{class_name}: {len(class_sd4m_roles)} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only checking the argument text probably does not give us much, but it shall serve as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate the labeling functions on the SD4M training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wsee.labeling import event_argument_role_lfs as role_lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the labeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "lfs = [\n",
    "    role_lfs.lf_location_adjacent_markers,\n",
    "    role_lfs.lf_location_adjacent_trigger_verb,\n",
    "    role_lfs.lf_location_beginning_street_stop_route,\n",
    "    role_lfs.lf_location_first_sentence_street_stop_route,\n",
    "    role_lfs.lf_location_first_sentence_priorities,\n",
    "    role_lfs.lf_delay_event_sentence,\n",
    "    role_lfs.lf_delay_preceding_arg,\n",
    "    role_lfs.lf_delay_preceding_trigger,\n",
    "    role_lfs.lf_direction_markers,\n",
    "    role_lfs.lf_direction_markers_order,\n",
    "    role_lfs.lf_direction_pattern,\n",
    "    role_lfs.lf_direction_markers_pattern_order,\n",
    "    role_lfs.lf_start_location_type,\n",
    "    role_lfs.lf_start_location_nearest,\n",
    "    role_lfs.lf_start_location_preceding_arg,\n",
    "    role_lfs.lf_end_location_type,\n",
    "    role_lfs.lf_end_location_nearest,\n",
    "    role_lfs.lf_end_location_preceding_arg,\n",
    "    role_lfs.lf_start_date_type,\n",
    "    role_lfs.lf_start_date_replicated,\n",
    "    role_lfs.lf_start_date_first,\n",
    "    role_lfs.lf_start_date_adjacent,\n",
    "    role_lfs.lf_end_date_type,\n",
    "    role_lfs.lf_end_date_replicated,\n",
    "    role_lfs.lf_cause_type,\n",
    "    role_lfs.lf_cause_replicated,\n",
    "    role_lfs.lf_cause_order,\n",
    "    role_lfs.lf_distance_type,\n",
    "    role_lfs.lf_distance_nearest,\n",
    "    role_lfs.lf_distance_order,\n",
    "    role_lfs.lf_route_type,\n",
    "    role_lfs.lf_route_type_order,\n",
    "    role_lfs.lf_route_type_order_between_check,\n",
    "    role_lfs.lf_delay_earlier_negative,\n",
    "    role_lfs.lf_date_negative,\n",
    "    role_lfs.lf_not_an_event,\n",
    "    role_lfs.lf_somajo_separate_sentence,\n",
    "    role_lfs.lf_overlapping,\n",
    "    role_lfs.lf_too_far_40,\n",
    "    role_lfs.lf_multiple_same_event_type,\n",
    "    role_lfs.lf_event_patterns,\n",
    "    role_lfs.lf_event_patterns_general_location\n",
    "]\n",
    "\n",
    "applier = PandasLFApplier(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_sd_train = applier.apply(df_sd_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "LFAnalysis(L_sd_train, lfs).lf_summary(Y_sd_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wsee.labeling import error_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_rows = labeled_sd4m_roles.iloc[L_sd_train[:, 0] == 0]\n",
    "print(len(relevant_rows))\n",
    "relevant_rows.sample()[['text', 'trigger', 'argument', 'label', 'event_types', 'event_arg_roles']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis.sample_fp(labeled_df=labeled_sd4m_roles, lf_outputs=L_sd_train, lf_index=0, label_of_interest=0, sample_size=1)[['between_tokens', 'trigger', 'argument', 'somajo_doc', 'label', 'event_types', 'event_arg_roles']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis.sample_abstained_instances(labeled_df=labeled_sd4m_roles, lf_outputs=L_sd_train, lf_index=19, label_of_interest=5, sample_size=1)[['text', 'between_tokens', 'trigger', 'argument', 'label', 'event_types', 'event_arg_roles']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis.sample_abstained_instances(labeled_df=labeled_sd4m_roles, lf_outputs=L_sd_train, lf_index=0, label_of_interest=0, sample_size=1)[['text', 'between_tokens', 'trigger', 'argument', 'label', 'event_types']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the Labeling model and label the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_file = DATA_DIR + '/pickled_daystream_role_examples'\n",
    "pickled_dataframe_file = Path(dataframe_file + '.pkl')\n",
    "\n",
    "df_daystream = None\n",
    "Y_daystream = None\n",
    "\n",
    "if pickled_dataframe_file.exists():\n",
    "    with open(pickled_dataframe_file, 'rb') as pickled_dataframe:\n",
    "        df_daystream, Y_daystream = pickle.load(pickled_dataframe)\n",
    "else:\n",
    "    df_daystream, Y_daystream = pipeline.build_event_role_examples(daystream)\n",
    "    with open(pickled_dataframe_file, 'wb') as pickled_dataframe:\n",
    "        pickle.dump((df_daystream, Y_daystream), pickled_dataframe)\n",
    "if 'event_roles' in df_daystream:\n",
    "    df_daystream.drop('event_roles', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_daystream = applier.apply(df_daystream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "LFAnalysis(L_daystream, lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelModel\n",
    "\n",
    "daystream_model = LabelModel(cardinality=11, verbose=True)\n",
    "daystream_model.fit(L_train=L_daystream, n_epochs=5000, log_freq=500, seed=12345, Y_dev=Y_sd_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daystream_model_acc = daystream_model.score(L=L_sd_train, Y=Y_sd_train, tie_break_policy=\"random\")[\n",
    "    \"accuracy\"\n",
    "]\n",
    "print(f\"{'Label Model Accuracy:':<25} {daystream_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daystream_probs = daystream_model.predict_proba(L=L_daystream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_daystream_with_abstains = pipeline.merge_event_role_examples(df_daystream, utils.zero_out_abstains(daystream_probs, L_daystream))\n",
    "labeled_daystream_with_abstains.reset_index(level=0).to_json(DATA_DIR + \"/save_daystreamv6_roles_with_abstains.jsonl\", orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Daystream Snorkel Labeling Check\n",
    "\n",
    "To look at the daystream labeling it would be best to remove the abstains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "df_daystream_filtered, probs_daystream_filtered = filter_unlabeled_dataframe(\n",
    "    X=df_daystream, y=daystream_probs, L=L_daystream\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daystream_filtered['role_probs'] = list(probs_daystream_filtered)\n",
    "df_daystream_filtered['most_probable_class'] = [ROLE_LABELS[label_idx] for label_idx in probs_daystream_filtered.argmax(axis=1)]\n",
    "df_daystream_filtered['max_class_prob'] = [\"{:.2f}\".format(class_prob) for class_prob in probs_daystream_filtered.max(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for role_class in ROLE_LABELS:\n",
    "    print(f\"{role_class}: {len(df_daystream_filtered[df_daystream_filtered['most_probable_class'] == role_class])} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daystream_filtered[df_daystream_filtered['most_probable_class'] == 'route'].sample(1)[['text', 'trigger', 'argument', 'most_probable_class', 'max_class_prob', 'role_probs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6 (wsee)",
   "language": "python",
   "name": "wsee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
