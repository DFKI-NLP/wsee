{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "For our experiments we prepared the data the following ways.\n",
    "\n",
    "1. We convert the corpus data from avro to jsonl using wsee/data/avro_to_jsonl.py & wsee/data/convert.py. The former converts n-ary relations into ACE style events. Th latter converts the n-ary relations into events in a specialized Snorkel format, i.e. turn string labels into class probabilities, while isolating the Daystream data from the corpus data.\n",
    "2. We develop labeling functions, learn label models with Snorkel and probabilistically label the Daystream data using wsee/data/pipeline.py. We do these 5 times with different seeds in order to perform random repeats.\n",
    "3. We create progressively bigger subsets (50%, 60%, 70%, 80%, 90%, 100%) from the probabilistically labeled data to examine whether more training data created using weak supervision improves model performance.\n",
    "\n",
    "In order to perform the next steps, we need to download the corpus data:\n",
    "https://cloud.dfki.de/owncloud/index.php/s/wSNN78s4Ck7omXm\n",
    "with\n",
    "\n",
    "```bash\n",
    "wget -O ../data/daystream_corpus.zip --content-disposition https://cloud.dfki.de/owncloud/index.php/s/L5igzCiLNxnM3HD/download\n",
    "unzip ../data/daystream_corpus.zip\n",
    "```\n",
    "\n",
    "Be sure to put the daystream_corpus into the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from pathlib import Path\n",
    "from wsee.data import avro_to_jsonl, convert, pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(\"../data/daystream_corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avro to jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion from avro to ACE stlye jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_to_jsonl.convert_avros(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion from avro to Snorkel style jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert.convert_avros(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snorkel labeling\n",
    "This probabilistically labels the daystream data using our labeling functions & learned Snorkel label models.\n",
    "It creates a merged version of the SD4M gold train data and the probabilistically labeled Daystream data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(\"../data/daystream_corpus\")\n",
    "seed = 12345\n",
    "pipeline.create_train_datasets(input_path, save_path, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random repeat variants\n",
    "This performs the Snorkel labeling steps #`random_repeat` times with different seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(\"../data/daystream_corpus\")\n",
    "random_repeats = 5\n",
    "pipeline.create_random_repeats_train_datasets(input_path, save_path, random_repeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daystream subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wsee.utils import corpus_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "daystream_snorkeled_path = Path(\"../data/daystream_corpus/daystream_snorkeled.jsonl\") \n",
    "output_path = Path(\"../data/daystream_corpus/\")\n",
    "daystream_snorkeled = pd.read_json(daystream_snorkeled_path, lines=True, encoding='utf8') \n",
    "sample_statistics = []\n",
    "for percentage in range(50, 101, 10):\n",
    "    row = {'sample_fraction': percentage}\n",
    "    sample = daystream_snorkeled.sample(frac=percentage/100)\n",
    "    print(f'{percentage}% sample statistics')\n",
    "    row.update(corpus_statistics.get_snorkel_event_stats(sample))\n",
    "    sample_statistics.append(row)\n",
    "    sample.to_json(output_path.joinpath(f\"daystream{percentage}_snorkeled.jsonl\"), orient='records', lines=True, force_ascii=False)\n",
    "sample_statistics = pd.DataFrame(sample_statistics)\n",
    "sample_statistics.to_json(output_path.joinpath('sample_statistics.jsonl'), orient='records', lines=True, force_ascii=False)\n",
    "sample_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALTERNATIVE: To reduce the chance of disproportionally getting more events per documents in some samples, we can alternatively create subsets for each of the random repeats and use the mean/median & standard deviation during the evaluation instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variant sample from each random repeat, which introduces even more randonmness via the seeds for the label models & eventx model in addition to the sample randomness. This did not work out well in past experiments.\n",
    "```python\n",
    "import pandas as pd\n",
    "input_path = Path(\"../data/daystream_corpus\")\n",
    "random_repeats = 5\n",
    "for run in range(random_repeats):\n",
    "    run_path = input_path.joinpath(f\"run_{run+1}\")\n",
    "    daystream_snorkeled = pd.read_json(run_path.joinpath('daystream_snorkeled.jsonl'), lines=True, encoding='utf8') \n",
    "    for percentage in range(50, 101, 10):\n",
    "        sample = daystream_snorkeled.sample(frac=percentage/100)\n",
    "        sample.to_json(run_path.joinpath(f\"daystream{percentage}_snorkeled.jsonl\"), orient='records', lines=True, force_ascii=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "input_path = Path(\"../data/daystream_corpus\")\n",
    "daystream_snorkeled_path = Path(\"../data/daystream_corpus/daystream_snorkeled.jsonl\") \n",
    "daystream_snorkeled = pd.read_json(daystream_snorkeled_path, lines=True, encoding='utf8') \n",
    "sample_repeats = 5\n",
    "for run in range(sample_repeats):\n",
    "    run_path = input_path.joinpath(f\"samples_{run+1}\")\n",
    "    for percentage in range(50, 101, 10):\n",
    "        sample = daystream_snorkeled.sample(frac=percentage/100)\n",
    "        sample_path = run_path.joinpath(f\"daystream{percentage}_snorkeled.jsonl\")\n",
    "        os.makedirs(os.path.dirname(sample_path), exist_ok=True)\n",
    "        sample.to_json(sample_path, orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SD4M Train Sample for experiments\n",
    "We further used a sample from the gold SD4M Train set to \n",
    "\n",
    "We also count all the event triggers and compare it to the number of event triggers in the data.\n",
    "We expect the latter to be lower as we converted n-ary relations into events, which excludes triggers with no arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sd4m_train = pd.read_json(\"../data/daystream_corpus/train/train_with_events_and_defaults.jsonl\", lines=True, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wsee.utils import corpus_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sd4m_train) # contains document with no trigger entity -> not relevant for event extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sd4m_train = sd4m_train[sd4m_train.apply(lambda document: corpus_statistics.has_triggers(document), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_statistics.get_snorkel_event_stats(filtered_sd4m_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = filtered_sd4m_train.sample(n=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_json(\"../data/daystream_corpus/train_sample.jsonl\", orient='records', lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6 (wsee)",
   "language": "python",
   "name": "wsee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
